{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqOvaNHA4xzZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression"
      ],
      "metadata": {
        "id": "XKe-oqDr423q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "\n",
        "# Start Spark\n",
        "spark = SparkSession.builder.appName(\"MLlibExamples\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "ZQJP1-ms5fzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "p05p5ULf5fL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Load dataset\n",
        "boston = datasets.load_diabetes()  # (Boston is deprecated, so using diabetes dataset for regression)\n",
        "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "y = pd.DataFrame(boston.target, columns=[\"label\"])\n",
        "df_pd = pd.concat([X, y], axis=1)\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "df = spark.createDataFrame(df_pd)\n",
        "\n",
        "# Assemble features\n",
        "assembler = VectorAssembler(inputCols=df.columns[:-1], outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Train regression model\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "model = lr.fit(df)\n",
        "\n",
        "print(\"Coefficients:\", model.coefficients)\n",
        "print(\"Intercept:\", model.intercept)\n",
        "print(\"RMSE:\", model.summary.rootMeanSquaredError)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFldCzaY5kwh",
        "outputId": "838d7ad7-719f-4a2a-ee0d-2ff322c9aef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [-10.009866299810275,-239.8156436724212,519.8459200544622,324.38464550232294,-792.1756385521954,476.7390210052281,101.04326793802068,177.06323767134498,751.2736995570891,67.62669218370446]\n",
            "Intercept: 152.13348416289597\n",
            "RMSE: 53.47612876402655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rktLiYB_5lU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Classification (Breast Cancer Dataset)"
      ],
      "metadata": {
        "id": "1ls-D4sa56rH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Load dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "y = pd.DataFrame(cancer.target, columns=[\"label\"])\n",
        "df_pd = pd.concat([X, y], axis=1)\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "df = spark.createDataFrame(df_pd)\n",
        "\n",
        "# Assemble features\n",
        "assembler = VectorAssembler(inputCols=df.columns[:-1], outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Train classifier\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
        "model = lr.fit(df)\n",
        "\n",
        "# Predict\n",
        "predictions = model.transform(df)\n",
        "predictions.select(\"label\", \"prediction\", \"probability\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWiJsjXQ58Y_",
        "outputId": "1067d747-e934-47c3-a1b5-32abdd298447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+--------------------+\n",
            "|label|prediction|         probability|\n",
            "+-----+----------+--------------------+\n",
            "|    0|       0.0|[0.99999999999986...|\n",
            "|    0|       0.0|[0.99999973030752...|\n",
            "|    0|       0.0|[0.99999999991389...|\n",
            "|    0|       0.0|[0.99997561772253...|\n",
            "|    0|       0.0|[0.99999979529657...|\n",
            "+-----+----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vl-yrNFq58-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "clustering Iris Dataset"
      ],
      "metadata": {
        "id": "kI0r7SRU6FmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df_pd = X.copy()\n",
        "df_pd[\"label\"] = iris.target\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "df = spark.createDataFrame(df_pd)\n",
        "\n",
        "# Assemble features\n",
        "assembler = VectorAssembler(inputCols=iris.feature_names, outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# KMeans clustering\n",
        "kmeans = KMeans(k=3, seed=1, featuresCol=\"features\")\n",
        "model = kmeans.fit(df)\n",
        "\n",
        "predictions = model.transform(df)\n",
        "predictions.select(\"features\", \"prediction\").show(5)\n",
        "\n",
        "print(\"Cluster Centers:\", model.clusterCenters())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qWVKkQy6K4X",
        "outputId": "3ce63a9f-124f-4438-8299-38101bc38bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+----------+\n",
            "|         features|prediction|\n",
            "+-----------------+----------+\n",
            "|[5.1,3.5,1.4,0.2]|         1|\n",
            "|[4.9,3.0,1.4,0.2]|         1|\n",
            "|[4.7,3.2,1.3,0.2]|         1|\n",
            "|[4.6,3.1,1.5,0.2]|         1|\n",
            "|[5.0,3.6,1.4,0.2]|         1|\n",
            "+-----------------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Cluster Centers: [array([5.88360656, 2.74098361, 4.38852459, 1.43442623]), array([5.006, 3.428, 1.462, 0.246]), array([6.85384615, 3.07692308, 5.71538462, 2.05384615])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7esUSFJn6Lc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced MLlib Techniques"
      ],
      "metadata": {
        "id": "pEfxvWCa6oeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator, ClusteringEvaluator\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MLlib_Examples\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "pTI25JS96pZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A41pTODZ6r7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression â€“ Diabetes Dataset"
      ],
      "metadata": {
        "id": "VLw4HsD69Bul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "# Load dataset\n",
        "diabetes = datasets.load_diabetes()\n",
        "X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
        "y = pd.DataFrame(diabetes.target, columns=[\"label\"])\n",
        "df_pd = pd.concat([X, y], axis=1)\n",
        "df = spark.createDataFrame(df_pd)\n",
        "\n",
        "# Assemble features\n",
        "assembler = VectorAssembler(inputCols=list(X.columns), outputCol=\"features\")\n",
        "df = assembler.transform(df).select(\"features\", \"label\")\n",
        "\n",
        "# Train/Test split\n",
        "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Model\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Hyperparameter tuning\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
        "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "             .build())\n",
        "\n",
        "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "\n",
        "cv = CrossValidator(estimator=lr,\n",
        "                    estimatorParamMaps=paramGrid,\n",
        "                    evaluator=evaluator,\n",
        "                    numFolds=5)\n",
        "\n",
        "cvModel = cv.fit(train)\n",
        "predictions = cvModel.transform(test)\n",
        "\n",
        "# Evaluation\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Best Model Params:\", cvModel.bestModel._java_obj.parent().extractParamMap())\n",
        "print(\"Test RMSE:\", rmse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO1nmxqH9CSl",
        "outputId": "9ce31dda-3f1f-4b78-d085-26a21202aa0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Params: {\n",
            "\tLinearRegression_cab2b80362de-aggregationDepth: 2,\n",
            "\tLinearRegression_cab2b80362de-elasticNetParam: 1.0,\n",
            "\tLinearRegression_cab2b80362de-epsilon: 1.35,\n",
            "\tLinearRegression_cab2b80362de-featuresCol: features,\n",
            "\tLinearRegression_cab2b80362de-fitIntercept: true,\n",
            "\tLinearRegression_cab2b80362de-labelCol: label,\n",
            "\tLinearRegression_cab2b80362de-loss: squaredError,\n",
            "\tLinearRegression_cab2b80362de-maxBlockSizeInMB: 0.0,\n",
            "\tLinearRegression_cab2b80362de-maxIter: 100,\n",
            "\tLinearRegression_cab2b80362de-predictionCol: prediction,\n",
            "\tLinearRegression_cab2b80362de-regParam: 0.01,\n",
            "\tLinearRegression_cab2b80362de-solver: auto,\n",
            "\tLinearRegression_cab2b80362de-standardization: true,\n",
            "\tLinearRegression_cab2b80362de-tol: 1.0E-6\n",
            "}\n",
            "Test RMSE: 59.96741272186755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZlKs5CTN9KXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification â€“ Breast Cancer Dataset"
      ],
      "metadata": {
        "id": "GLocNXUy9TU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Load dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "y = pd.DataFrame(cancer.target, columns=[\"label\"])\n",
        "df_pd = pd.concat([X, y], axis=1)\n",
        "df = spark.createDataFrame(df_pd)\n",
        "\n",
        "# Assemble features\n",
        "assembler = VectorAssembler(inputCols=list(X.columns), outputCol=\"features\")\n",
        "df = assembler.transform(df).select(\"features\", \"label\")\n",
        "\n",
        "# Train/Test split\n",
        "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=20)\n",
        "\n",
        "# Hyperparameter tuning\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(lr.regParam, [0.01, 0.1])\n",
        "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "             .build())\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "cv = CrossValidator(estimator=lr,\n",
        "                    estimatorParamMaps=paramGrid,\n",
        "                    evaluator=evaluator,\n",
        "                    numFolds=5)\n",
        "\n",
        "cvModel = cv.fit(train)\n",
        "predictions = cvModel.transform(test)\n",
        "\n",
        "# Evaluation\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Best Model Params:\", cvModel.bestModel._java_obj.parent().extractParamMap())\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzuhKawq9T-U",
        "outputId": "57fed38c-6cd1-4dde-a36e-63b5ec34ffd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Params: {\n",
            "\tLogisticRegression_9cdfd2648582-aggregationDepth: 2,\n",
            "\tLogisticRegression_9cdfd2648582-elasticNetParam: 0.0,\n",
            "\tLogisticRegression_9cdfd2648582-family: auto,\n",
            "\tLogisticRegression_9cdfd2648582-featuresCol: features,\n",
            "\tLogisticRegression_9cdfd2648582-fitIntercept: true,\n",
            "\tLogisticRegression_9cdfd2648582-labelCol: label,\n",
            "\tLogisticRegression_9cdfd2648582-maxBlockSizeInMB: 0.0,\n",
            "\tLogisticRegression_9cdfd2648582-maxIter: 20,\n",
            "\tLogisticRegression_9cdfd2648582-predictionCol: prediction,\n",
            "\tLogisticRegression_9cdfd2648582-probabilityCol: probability,\n",
            "\tLogisticRegression_9cdfd2648582-rawPredictionCol: rawPrediction,\n",
            "\tLogisticRegression_9cdfd2648582-regParam: 0.01,\n",
            "\tLogisticRegression_9cdfd2648582-standardization: true,\n",
            "\tLogisticRegression_9cdfd2648582-threshold: 0.5,\n",
            "\tLogisticRegression_9cdfd2648582-tol: 1.0E-6\n",
            "}\n",
            "Test Accuracy: 0.9789473684210527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WLCUnwWf9V-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering â€“ Iris Dataset"
      ],
      "metadata": {
        "id": "WplFzbNB9bBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df_pd = X.copy()\n",
        "df_pd[\"label\"] = iris.target\n",
        "df = spark.createDataFrame(df_pd)\n",
        "\n",
        "# Assemble features\n",
        "assembler = VectorAssembler(inputCols=iris.feature_names, outputCol=\"features\")\n",
        "df = assembler.transform(df).select(\"features\", \"label\")\n",
        "\n",
        "# Train/Test split (not typical in clustering, but we can still do it for evaluation)\n",
        "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Model\n",
        "kmeans = KMeans(featuresCol=\"features\", k=3, seed=42)\n",
        "\n",
        "# Hyperparameter tuning (try different cluster numbers)\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(kmeans.k, [2, 3, 4, 5])\n",
        "             .build())\n",
        "\n",
        "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n",
        "\n",
        "cv = CrossValidator(estimator=kmeans,\n",
        "                    estimatorParamMaps=paramGrid,\n",
        "                    evaluator=evaluator,\n",
        "                    numFolds=3)\n",
        "\n",
        "cvModel = cv.fit(train)\n",
        "predictions = cvModel.transform(test)\n",
        "\n",
        "# Evaluation\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Best k:\", cvModel.bestModel.summary.k)\n",
        "print(\"Silhouette Score:\", silhouette)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ldb6AQQ9dKB",
        "outputId": "f1c29be3-91c0-44a8-a76d-bef1ae000012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best k: 2\n",
            "Silhouette Score: 0.8885470258075133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2lLOzFqy9d5d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}